import argparse
import torch
import os
import json
import sys
sys.path.append('/your_path/ModalPrompt')
from tqdm import tqdm
import shortuuid
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig
from llava.constants import DEFAULT_IMAGE_PATCH_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
from llava.conversation import conv_templates, SeparatorStyle
from llava.model.builder import load_pretrained_model
from llava.utils import disable_torch_init
from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria
from llava.model import *
from PIL import Image
import math
template = "Choose an answer from the choices below: Doormat, Pomeranian, Chime, Golden retriever, Garden spider, Piggy bank, Walker hound, Castle, Chimpanzee, Sunscreen, Projectile, Accordion, Hand blower, Stupa, Kimono, German shepherd, Mouse, Maillot, Rotisserie, Earthstar, Television, Banjo, Jaguar, Cock, Goblet, Organ, Mortarboard, Hard disc, Red-backed sandpiper, Valley, Bow tie, Desk, Shopping basket, Marmoset, Mantis, Tiger beetle, Meat loaf, Curly-coated retriever, American black bear, Hyena, Spatula, Toaster, Cucumber, Espresso maker, Irish terrier, Fig, Tennis ball, Thatch, Spotted salamander, Dandie dinmont, Dalmatian, Sealyham terrier, Jack-o'-lantern, Hamper, Eggnog, Gordon setter, Water ouzel, Afghan hound, Sloth bear, Teapot, Standard poodle, Sunglass, Leafhopper, Barometer, Recreational vehicle, Cabbage butterfly, Poncho, Lampshade, Agaric, Koala, African crocodile, Envelope, Carpenter's kit, Old english sheepdog, Chocolate sauce, Dough, Bucket, Microphone, Lorikeet, Paddle, Crane2, Hip, Quail, Pickup, Beer glass, Face powder, Rottweiler, Tub, Head cabbage, Swing, Malamute, Damselfly, Hartebeest, Gondola, Hog, Web site, Whippet, Gasmask, Lemon, Bernese mountain dog."

def split_list(lst, n):
    """Split a list into n (roughly) equal-sized chunks"""
    chunk_size = math.ceil(len(lst) / n)  # integer division
    return [lst[i:i+chunk_size] for i in range(0, len(lst), chunk_size)]


def get_chunk(lst, n, k):
    chunks = split_list(lst, n)
    return chunks[k]


def eval_model(args):
    # Model
    disable_torch_init()
    model_path = os.path.expanduser(args.model_path)
    model_name = get_model_name_from_path(model_path)

    tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, args.model_base, model_name, args.prefix_len, args.cur_task, args.text_tower, args.num_tasks)

    with open(os.path.expanduser(args.question_file), "r") as f:
        questions = json.load(f)
    questions = get_chunk(questions, args.num_chunks, args.chunk_idx)
    
    answers_file = os.path.expanduser(args.answers_file)
    os.makedirs(os.path.dirname(answers_file), exist_ok=True)
    ans_file = open(answers_file, "w")
    count = 0 
    for line in tqdm(questions):
        count += 1
        idx = line["question_id"]
        image_file = line["image"]
        qs = line["text"]

        cur_prompt = qs
        if model.config.mm_use_im_start_end:
            qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + qs
        else:
            qs = DEFAULT_IMAGE_TOKEN + '\n' + qs

        conv = conv_templates[args.conv_mode].copy()
        conv.append_message(conv.roles[0], qs)
        conv.append_message(conv.roles[1], None)
        prompt = conv.get_prompt()

        input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).cuda()

        image = Image.open(os.path.join(args.image_folder, image_file))
        image_tensor = image_processor.preprocess(image, return_tensors='pt')['pixel_values'][0]

        stop_str = conv.sep if conv.sep_style != SeparatorStyle.TWO else conv.sep2
        keywords = [stop_str]
        stopping_criteria = KeywordsStoppingCriteria(keywords, tokenizer, input_ids)

        with torch.inference_mode():
            output_ids = model.generate(
                input_ids,
                images=image_tensor.unsqueeze(0).half().cuda(),
                do_sample=True if args.temperature > 0 else False,
                temperature=args.temperature,
                top_p=args.top_p,
                num_beams=args.num_beams,
                # no_repeat_ngram_size=3,
                max_new_tokens=128,
                use_cache=True)

        input_token_len = input_ids.shape[1]
        n_diff_input_output = (input_ids != output_ids[:, :input_token_len]).sum().item()
        if n_diff_input_output > 0:
            print(f'[Warning] {n_diff_input_output} output_ids are not the same as the input_ids')
        outputs = tokenizer.batch_decode(output_ids[:, input_token_len:], skip_special_tokens=True)[0]
        outputs = outputs.strip()
        if outputs.endswith(stop_str):
            outputs = outputs[:-len(stop_str)]
        outputs = outputs.strip()
        
        ans_id = shortuuid.uuid()
        ans_file.write(json.dumps({"question_id": idx,
                                   "prompt": cur_prompt,
                                   "text": outputs,
                                   "answer_id": ans_id,
                                   "model_id": model_name,
                                   "metadata": {}}) + "\n")
        ans_file.flush()
    ans_file.close()

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-path", type=str, default="facebook/opt-350m")
    parser.add_argument("--model-base", type=str, default=None)
    parser.add_argument("--image-folder", type=str, default="")
    parser.add_argument("--question-file", type=str, default="tables/question.jsonl")
    parser.add_argument("--answers-file", type=str, default="answer.jsonl")
    parser.add_argument("--conv-mode", type=str, default="llava_v1")
    parser.add_argument("--num-chunks", type=int, default=1)
    parser.add_argument("--chunk-idx", type=int, default=0)
    parser.add_argument("--temperature", type=float, default=0)
    parser.add_argument("--top_p", type=float, default=None)
    parser.add_argument("--num_beams", type=int, default=1)
    parser.add_argument("--prefix-len", type=int, default=10)
    parser.add_argument("--cur-task", type=int, default=1)
    parser.add_argument("--num-tasks", type=int, default=10)
    parser.add_argument("--text-tower",type=str, default="CLIP")
    
    args = parser.parse_args()

    eval_model(args)
